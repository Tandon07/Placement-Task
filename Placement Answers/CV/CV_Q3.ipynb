{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup as bs, BeautifulSoup\n",
        "\n",
        "driver = webdriver.Chrome(ChromeDriverManager().install())"
      ],
      "metadata": {
        "id": "kmcdYmKVEb_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping for train data"
      ],
      "metadata": {
        "id": "U3DAcM9UEb9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=['shirt','jeans','top','frok','hoodie']\n",
        "\n",
        "    \n",
        "\n",
        "for i in l:\n",
        "\n",
        "    os.makedirs(i, exist_ok=True)\n",
        "\n",
        "      \n",
        "    # URL of the e-commerce website\n",
        "   \n",
        "    url = f\"https://www.amazon.in/s?k={i}\"\n",
        "        # Send a GET request to the website\n",
        "    driver.get(url)\n",
        "    last_height = 200\n",
        "    start_height = 0\n",
        "    \n",
        "    while last_height < 3000:\n",
        "        driver.execute_script(\"window.scrollTo(\" + str(start_height) + \", \" + str(last_height) + \");\")\n",
        "        start_height = last_height\n",
        "        last_height = last_height + 400\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "          # Create a BeautifulSoup object to parse the HTML content\n",
        "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    image_elements = bs.select(\".s-image\")\n",
        "    c=0\n",
        "    c_max=67\n",
        "    for img in image_elements:\n",
        "              # Get the source URL of the image\n",
        "        img_url = img[\"src\"]\n",
        "\n",
        "        # Save the image to the output directory\n",
        "        download_image(img_url, f'{i}/', f\"{c}\")\n",
        "        \n",
        "        if c>c_max:\n",
        "            break\n",
        "        c=c+1\n",
        "        print(f\"Image '{c}' downloaded and saved.\")\n",
        "\n",
        "    print(\"Image scraping completed.\")"
      ],
      "metadata": {
        "id": "PYQCk--bEb7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping for test data"
      ],
      "metadata": {
        "id": "XOOEZeDkEb5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=['shirt','jeans','top','frok','hoodie']\n",
        "\n",
        "\n",
        "for i in l:\n",
        "\n",
        "    os.makedirs(i, exist_ok=True)\n",
        "      \n",
        "    url = url = f\"https://www.myntra.com/{i}?rawQuery={i}\"\n",
        "        # Send a GET request to the website\n",
        "    driver.get(url)\n",
        "    last_height = 200\n",
        "    start_height = 0\n",
        "\n",
        "\n",
        "          # Create a BeautifulSoup object to parse the HTML content\n",
        "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    image_elements = bs.select(\".product-imageSliderContainer .img-responsive img\")\n",
        "    c=0\n",
        "    c_max=10\n",
        "    for img in image_elements:\n",
        "              # Get the source URL of the image\n",
        "        img_url = img[\"src\"]\n",
        "\n",
        "        # Save the image to the output directory\n",
        "\n",
        "        download_image(img_url, f'{i}/', f\"{c}\")\n",
        "        \n",
        "        if c>c_max:\n",
        "            break\n",
        "        c=c+1\n",
        "        print(f\"Image '{c}' downloaded and saved.\")\n",
        "\n",
        "    print(\"Image scraping completed.\")"
      ],
      "metadata": {
        "id": "lMtO7e0WEb2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scraping for validation"
      ],
      "metadata": {
        "id": "9OTfLAsJEb0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l=['shirt','jeans','top','frok','hoodie']\n",
        "\n",
        "\n",
        "driver.maximize_window()   \n",
        "    \n",
        "\n",
        "for i in l:\n",
        "\n",
        "    os.makedirs(i, exist_ok=True)\n",
        "      \n",
        "    url = f\"https://www.ajio.com/search/?text={i}\"\n",
        "        # Send a GET request to the website\n",
        "    driver.get(url)\n",
        "    \n",
        "    last_height = 200\n",
        "    start_height = 0\n",
        "    \n",
        "    while last_height < 1000:\n",
        "        driver.execute_script(\"window.scrollTo(\" + str(start_height) + \", \" + str(last_height) + \");\")\n",
        "        start_height = last_height\n",
        "        last_height = last_height + 200\n",
        "        time.sleep(2)\n",
        "\n",
        "          # Create a BeautifulSoup object to parse the HTML content\n",
        "    bs = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    image_elements = bs.select(\"div .preview img\")\n",
        "    c=0\n",
        "    c_max=10\n",
        "    for img in image_elements:\n",
        "              # Get the source URL of the image\n",
        "        img_url = img[\"src\"]\n",
        "\n",
        "        # Save the image to the output directory\n",
        "\n",
        "        download_image(img_url, f'{i}/', f\"{c}\")\n",
        "        \n",
        "        if c>c_max:\n",
        "            break\n",
        "        c=c+1\n",
        "        print(f\"Image '{c}' downloaded and saved.\")\n",
        "    \n",
        "\n",
        "    print(\"Image scraping completed.\")"
      ],
      "metadata": {
        "id": "RjJgErCCEbvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  NOW doing Image Classification"
      ],
      "metadata": {
        "id": "4-ns5Y0yEbtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I have scraped less number of images since uploading on gdrive was taking too much time."
      ],
      "metadata": {
        "id": "REqQojLqQ8h0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as transforms\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "kQdKK1WXgCr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE-_hQregCuu",
        "outputId": "06871d1c-d499-40b8-f6bf-365c45bbbe8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/CV Q3 dataset/images\""
      ],
      "metadata": {
        "id": "dts9LWkvWghB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Set the path to your dataset folder\n",
        "train_dir = '/content/drive/MyDrive/CV Q3 dataset/images/train'\n",
        "test_dir = '/content/drive/MyDrive/CV Q3 dataset/images/test'\n",
        "val_dir = '/content/drive/MyDrive/CV Q3 dataset/images/validation'\n",
        "\n",
        "\n",
        "# Set the hyperparameters for training\n",
        "num_classes = 5\n",
        "batch_size = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(224, 224),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu7YtSIOX4fJ",
        "outputId": "ebeb39b2-4e14-41d5-bd8e-cdf175e45aec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 335 files belonging to 5 classes.\n",
            "Found 55 files belonging to 5 classes.\n",
            "Found 55 files belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QekpxwuG560m",
        "outputId": "57c660fd-e4cd-4ae4-a7cb-c10ad2fb979b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['frok', 'hoodie', 'jeans', 'shirt', 'top']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the dataset for performance\n",
        "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "\n",
        "# Define the custom model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Conv2D(16, kernel_size=3, activation='relu', input_shape=(224, 224, 3)),\n",
        "    layers.MaxPooling2D(pool_size=2),\n",
        "    layers.Conv2D(32, kernel_size=3, activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=num_epochs\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ygx4UbH3rEq",
        "outputId": "fbfe86e1-8ef1-4a6d-8d0c-ed2161604a39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11/11 [==============================] - 41s 4s/step - loss: 789.1993 - accuracy: 0.2000 - val_loss: 41.0209 - val_accuracy: 0.2000\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 17s 1s/step - loss: 10.9567 - accuracy: 0.5075 - val_loss: 2.9146 - val_accuracy: 0.4909\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 18s 2s/step - loss: 0.9507 - accuracy: 0.8358 - val_loss: 3.0517 - val_accuracy: 0.4545\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.1277 - accuracy: 0.9761 - val_loss: 2.9934 - val_accuracy: 0.4909\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 17s 1s/step - loss: 0.0662 - accuracy: 0.9970 - val_loss: 3.2135 - val_accuracy: 0.5091\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 16s 1s/step - loss: 0.0339 - accuracy: 0.9970 - val_loss: 3.7948 - val_accuracy: 0.5091\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 17s 1s/step - loss: 0.0324 - accuracy: 0.9910 - val_loss: 3.4807 - val_accuracy: 0.4545\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 15s 1s/step - loss: 0.0269 - accuracy: 0.9970 - val_loss: 3.1171 - val_accuracy: 0.5091\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 21s 2s/step - loss: 0.0180 - accuracy: 0.9970 - val_loss: 2.8682 - val_accuracy: 0.5091\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 17s 1s/step - loss: 0.0082 - accuracy: 0.9970 - val_loss: 3.5666 - val_accuracy: 0.5091\n",
            "2/2 [==============================] - 1s 259ms/step - loss: 3.1381 - accuracy: 0.4364\n",
            "Test Loss: 3.138050079345703, Test Accuracy: 0.4363636374473572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform inference on a new image\n",
        "image_path = '/content/drive/MyDrive/CV Q3 dataset/images/validation/jeans/9.jpg'\n",
        "image = tf.keras.preprocessing.image.load_img(image_path, target_size=(224, 224))\n",
        "image = tf.keras.preprocessing.image.img_to_array(image)\n",
        "image = tf.expand_dims(image, 0)\n",
        "prediction = model.predict(image)\n",
        "predicted_class = tf.argmax(prediction, axis=1)\n",
        "print(f\"Predicted Class: {predicted_class[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRbu7p44X4hl",
        "outputId": "89719874-cf69-440c-ff9e-51d3ee6b8e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 66ms/step\n",
            "Predicted Class: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5zcJSBRWcq2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Za1rOOZfC0cH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}